{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d29b130-bb82-46af-b2b1-a83de0bdae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7850 (30.66 KB)\n",
      "Trainable params: 7850 (30.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.4205 - accuracy: 0.6465 - val_loss: 0.9087 - val_accuracy: 0.8216\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8024 - accuracy: 0.8231 - val_loss: 0.6629 - val_accuracy: 0.8557\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.8479 - val_loss: 0.5648 - val_accuracy: 0.8677\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5749 - accuracy: 0.8600 - val_loss: 0.5115 - val_accuracy: 0.8770\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5300 - accuracy: 0.8671 - val_loss: 0.4768 - val_accuracy: 0.8818\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4993 - accuracy: 0.8726 - val_loss: 0.4524 - val_accuracy: 0.8857\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4766 - accuracy: 0.8769 - val_loss: 0.4342 - val_accuracy: 0.8888\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.8796 - val_loss: 0.4197 - val_accuracy: 0.8919\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.8827 - val_loss: 0.4081 - val_accuracy: 0.8938\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.8847 - val_loss: 0.3985 - val_accuracy: 0.8963\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8867 - val_loss: 0.3901 - val_accuracy: 0.8974\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8888 - val_loss: 0.3833 - val_accuracy: 0.8987\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4071 - accuracy: 0.8900 - val_loss: 0.3770 - val_accuracy: 0.8992\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8916 - val_loss: 0.3715 - val_accuracy: 0.9012\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8930 - val_loss: 0.3667 - val_accuracy: 0.9017\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8937 - val_loss: 0.3624 - val_accuracy: 0.9028\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3847 - accuracy: 0.8954 - val_loss: 0.3585 - val_accuracy: 0.9038\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8963 - val_loss: 0.3549 - val_accuracy: 0.9043\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3764 - accuracy: 0.8968 - val_loss: 0.3517 - val_accuracy: 0.9044\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8975 - val_loss: 0.3487 - val_accuracy: 0.9052\n",
      "313/313 [==============================] - 0s 692us/step - loss: 0.3492 - accuracy: 0.9076\n",
      "Test score: 0.3491736352443695\n",
      "Test accuracy: 0.9075999855995178\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()  # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved\n",
    "\n",
    "# data: shuffled and split between train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6bb6b46-c765-40a8-aed3-62ad7fc33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.4775 - accuracy: 0.6094 - val_loss: 0.7590 - val_accuracy: 0.8353\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5958 - accuracy: 0.8519 - val_loss: 0.4515 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4347 - accuracy: 0.8815 - val_loss: 0.3721 - val_accuracy: 0.8977\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3768 - accuracy: 0.8939 - val_loss: 0.3331 - val_accuracy: 0.9057\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3445 - accuracy: 0.9019 - val_loss: 0.3121 - val_accuracy: 0.9117\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3222 - accuracy: 0.9077 - val_loss: 0.2941 - val_accuracy: 0.9157\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3046 - accuracy: 0.9137 - val_loss: 0.2800 - val_accuracy: 0.9198\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2905 - accuracy: 0.9176 - val_loss: 0.2687 - val_accuracy: 0.9224\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2778 - accuracy: 0.9205 - val_loss: 0.2589 - val_accuracy: 0.9249\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2666 - accuracy: 0.9239 - val_loss: 0.2499 - val_accuracy: 0.9281\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2566 - accuracy: 0.9267 - val_loss: 0.2433 - val_accuracy: 0.9294\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2476 - accuracy: 0.9292 - val_loss: 0.2343 - val_accuracy: 0.9320\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2387 - accuracy: 0.9324 - val_loss: 0.2285 - val_accuracy: 0.9342\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2306 - accuracy: 0.9341 - val_loss: 0.2211 - val_accuracy: 0.9371\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2232 - accuracy: 0.9359 - val_loss: 0.2146 - val_accuracy: 0.9388\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9383 - val_loss: 0.2097 - val_accuracy: 0.9408\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2091 - accuracy: 0.9403 - val_loss: 0.2040 - val_accuracy: 0.9431\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9419 - val_loss: 0.1991 - val_accuracy: 0.9440\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1967 - accuracy: 0.9440 - val_loss: 0.1944 - val_accuracy: 0.9473\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1910 - accuracy: 0.9456 - val_loss: 0.1905 - val_accuracy: 0.9467\n",
      "313/313 [==============================] - 0s 845us/step - loss: 0.1889 - accuracy: 0.9461\n",
      "Test score: 0.18886294960975647\n",
      "Test accuracy: 0.9460999965667725\n"
     ]
    }
   ],
   "source": [
    "# We can see that the baseline in the cell above was 90.75% accuracy - let's improve that....\n",
    "# We're going to add Hidden Layers afater the input layer...\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()  # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved\n",
    "\n",
    "# data: shuffled and split between train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f271dc6-e52e-43e2-8816-f2aa21d69a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.4664 - accuracy: 0.6266 - val_loss: 0.7438 - val_accuracy: 0.8368\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.6001 - accuracy: 0.8505 - val_loss: 0.4598 - val_accuracy: 0.8820\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4446 - accuracy: 0.8807 - val_loss: 0.3805 - val_accuracy: 0.8978\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3846 - accuracy: 0.8944 - val_loss: 0.3422 - val_accuracy: 0.9046\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3511 - accuracy: 0.9017 - val_loss: 0.3174 - val_accuracy: 0.9100\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3281 - accuracy: 0.9080 - val_loss: 0.3002 - val_accuracy: 0.9147\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3105 - accuracy: 0.9126 - val_loss: 0.2861 - val_accuracy: 0.9179\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2962 - accuracy: 0.9167 - val_loss: 0.2748 - val_accuracy: 0.9203\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2835 - accuracy: 0.9197 - val_loss: 0.2652 - val_accuracy: 0.9243\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2726 - accuracy: 0.9230 - val_loss: 0.2557 - val_accuracy: 0.9257\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2625 - accuracy: 0.9251 - val_loss: 0.2479 - val_accuracy: 0.9288\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2531 - accuracy: 0.9285 - val_loss: 0.2419 - val_accuracy: 0.9302\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2448 - accuracy: 0.9303 - val_loss: 0.2352 - val_accuracy: 0.9326\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2368 - accuracy: 0.9326 - val_loss: 0.2264 - val_accuracy: 0.9351\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2292 - accuracy: 0.9348 - val_loss: 0.2207 - val_accuracy: 0.9370\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2222 - accuracy: 0.9366 - val_loss: 0.2148 - val_accuracy: 0.9379\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2154 - accuracy: 0.9390 - val_loss: 0.2095 - val_accuracy: 0.9405\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2092 - accuracy: 0.9403 - val_loss: 0.2058 - val_accuracy: 0.9420\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2032 - accuracy: 0.9415 - val_loss: 0.1994 - val_accuracy: 0.9432\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9438 - val_loss: 0.1940 - val_accuracy: 0.9454\n",
      "313/313 [==============================] - 0s 854us/step - loss: 0.1971 - accuracy: 0.9439\n",
      "Test score: 0.1970636248588562\n",
      "Test accuracy: 0.9438999891281128\n"
     ]
    }
   ],
   "source": [
    "# From the cell above, you can see the accuracy incresed now to 94.61%\n",
    "# Let's try to improve this more with regularization (dropping a few values)\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()  # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved\n",
    "\n",
    "DROPOUT = 0.3 # Adding this here to try and improve this cell by randomly dropping a few values.\n",
    "\n",
    "# data: shuffled and split between train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3564e8-0470-4f45-8b7c-48de38cc243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.3421 - accuracy: 0.9018 - val_loss: 0.1881 - val_accuracy: 0.9464\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1449 - accuracy: 0.9557 - val_loss: 0.1275 - val_accuracy: 0.9613\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1014 - accuracy: 0.9693 - val_loss: 0.1029 - val_accuracy: 0.9683\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0740 - accuracy: 0.9779 - val_loss: 0.1003 - val_accuracy: 0.9713\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0597 - accuracy: 0.9819 - val_loss: 0.0975 - val_accuracy: 0.9728\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0470 - accuracy: 0.9853 - val_loss: 0.0991 - val_accuracy: 0.9736\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9882 - val_loss: 0.0917 - val_accuracy: 0.9758\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0305 - accuracy: 0.9902 - val_loss: 0.0937 - val_accuracy: 0.9766\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0250 - accuracy: 0.9925 - val_loss: 0.0964 - val_accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.1057 - val_accuracy: 0.9758\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.1001 - val_accuracy: 0.9780\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.0993 - val_accuracy: 0.9793\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.1189 - val_accuracy: 0.9755\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.1161 - val_accuracy: 0.9770\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.1223 - val_accuracy: 0.9757\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.1192 - val_accuracy: 0.9772\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.1271 - val_accuracy: 0.9783\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.1307 - val_accuracy: 0.9773\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.1389 - val_accuracy: 0.9768\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.1379 - val_accuracy: 0.9779\n",
      "313/313 [==============================] - 0s 855us/step - loss: 0.1093 - accuracy: 0.9791\n",
      "Test score: 0.10932628065347672\n",
      "Test accuracy: 0.9790999889373779\n"
     ]
    }
   ],
   "source": [
    "# This actually lowered the accuracy a bit to 94.38% \n",
    "# Let's trythe other optimizers (RMSprop and Adam)\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam # New optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = RMSprop() # New optimizer to try\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved\n",
    "\n",
    "DROPOUT = 0.3 # Adding this here to try and improve this cell by randomly dropping a few values.\n",
    "\n",
    "# data: shuffled and split between train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0776c3-5fd2-416b-9af9-ac4b8185ca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3511 - accuracy: 0.9002 - val_loss: 0.1732 - val_accuracy: 0.9519\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1447 - accuracy: 0.9567 - val_loss: 0.1302 - val_accuracy: 0.9625\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1002 - accuracy: 0.9702 - val_loss: 0.1122 - val_accuracy: 0.9677\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0735 - accuracy: 0.9780 - val_loss: 0.0991 - val_accuracy: 0.9708\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.9826 - val_loss: 0.0907 - val_accuracy: 0.9753\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0482 - accuracy: 0.9852 - val_loss: 0.0900 - val_accuracy: 0.9754\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0361 - accuracy: 0.9888 - val_loss: 0.0868 - val_accuracy: 0.9762\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0296 - accuracy: 0.9909 - val_loss: 0.0910 - val_accuracy: 0.9758\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0234 - accuracy: 0.9928 - val_loss: 0.0934 - val_accuracy: 0.9741\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1008 - val_accuracy: 0.9737\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9946 - val_loss: 0.0952 - val_accuracy: 0.9763\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0972 - val_accuracy: 0.9772\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0954 - val_accuracy: 0.9769\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.1063 - val_accuracy: 0.9754\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.1117 - val_accuracy: 0.9753\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.1185 - val_accuracy: 0.9732\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.1215 - val_accuracy: 0.9745\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.1335 - val_accuracy: 0.9723\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.1155 - val_accuracy: 0.9765\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.1063 - val_accuracy: 0.9791\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9789\n",
      "Test score: 0.10289924591779709\n",
      "Test accuracy: 0.9789000153541565\n"
     ]
    }
   ],
   "source": [
    "# Using the RMSprop Optimizer, the accuracy has now climbed to 97.91%~~!!!\n",
    "# Let's see how Adam does\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam # New optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = Adam() # New optimizer to try\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved\n",
    "\n",
    "DROPOUT = 0.3 # Adding this here to try and improve this cell by randomly dropping a few values.\n",
    "\n",
    "# data: shuffled and split between train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469acb39-1ad1-49c2-984b-7a9f40a0fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669706 (2.55 MB)\n",
      "Trainable params: 669706 (2.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 3s 6ms/step - loss: 0.2588 - accuracy: 0.9200 - val_loss: 0.1198 - val_accuracy: 0.9612\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0944 - accuracy: 0.9709 - val_loss: 0.0927 - val_accuracy: 0.9721\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9814 - val_loss: 0.0953 - val_accuracy: 0.9696\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.0929 - val_accuracy: 0.9736\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0285 - accuracy: 0.9910 - val_loss: 0.0847 - val_accuracy: 0.9764\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.0893 - val_accuracy: 0.9783\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0153 - accuracy: 0.9952 - val_loss: 0.0898 - val_accuracy: 0.9793\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0888 - val_accuracy: 0.9804\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0989 - val_accuracy: 0.9793\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.1165 - val_accuracy: 0.9783\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.1061 - val_accuracy: 0.9801\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.1181 - val_accuracy: 0.9784\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.1085 - val_accuracy: 0.9802\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.1336 - val_accuracy: 0.9783\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1093 - val_accuracy: 0.9815\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.1234 - val_accuracy: 0.9814\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 9.0764e-04 - accuracy: 0.9997 - val_loss: 0.1169 - val_accuracy: 0.9824\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 9.5220e-05 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9827\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 8.7461e-05 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9825\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.9128e-05 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9831\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9843\n",
      "Test score: 0.09717399626970291\n",
      "Test accuracy: 0.9843000173568726\n"
     ]
    }
   ],
   "source": [
    "# Adam came in JUST below the last with an accuracy of 97.89%......\n",
    "# Let's use the RMSprop optimizer and change the number of NHIDDEN now and the MNIST rate....\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam # New optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = RMSprop() # New optimizer to try\n",
    "N_HIDDEN = 512\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved\n",
    "\n",
    "DROPOUT = 0.3 # Adding this here to try and improve this cell by randomly dropping a few values.\n",
    "\n",
    "# data: shuffled and split between train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ed81c-c0ad-4f38-94aa-205e0a49e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Accuracy Changes in Neural Network Experiments\n",
    "\n",
    "In Steps 3 and 4, we conducted a series of experiments to understand the impact of various configurations on the accuracy \n",
    "rates of our neural network, which was trained on the MNIST dataset for digit classification.\n",
    "\n",
    "## Step 3: Hidden Layers Addition\n",
    "\n",
    "In this step, we added more hidden layers to the baseline model. The key changes were:\n",
    "\n",
    "- **Number of Hidden Layers:** 2\n",
    "- **Accuracy:** Increased from the baseline of 90.75% to 94.38%\n",
    "\n",
    "The addition of hidden layers allowed the model to capture more complex patterns in the data, leading to an improvement in accuracy.\n",
    "\n",
    "## Step 4: Optimizer Comparison and Hyperparameter Tuning\n",
    "\n",
    "### RMSprop vs. Adam (Optimizer Comparison)\n",
    "\n",
    "We compared the performance of two optimizers, RMSprop and Adam:\n",
    "\n",
    "- **RMSprop Accuracy:** 97.91%\n",
    "- **Adam Accuracy:** 97.89%\n",
    "\n",
    "Both optimizers performed similarly, showcasing the robustness of the model to different optimization algorithms.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "In this step, we performed hyperparameter tuning by changing the number of hidden units and other parameters:\n",
    "\n",
    "- **Optimizer:** RMSprop\n",
    "- **Number of Hidden Units:** Increased to 512\n",
    "- **Accuracy:** Further increased to 98.43%\n",
    "\n",
    "Increasing the number of hidden units enhanced the model's capacity to learn intricate features, resulting in a higher accuracy rate.\n",
    "\n",
    "## Overall Observations\n",
    "\n",
    "- **Accuracy Rates:**\n",
    "  - Baseline: 90.75%\n",
    "  - Hidden Layers Addition: 94.38%\n",
    "  - RMSprop vs. Adam: Comparable performance\n",
    "  - Hyperparameter Tuning: Significant improvement to 98.43%\n",
    "\n",
    "- **Training, Validation, and Test Data Sets:**\n",
    "  - Training Accuracy: Generally increased across all steps.\n",
    "  - Validation Accuracy: Improved with additional layers and hyperparameter tuning.\n",
    "  - Test Accuracy: Consistently reflected the improvements made during experimentation.\n",
    "\n",
    "These changes in accuracy rates highlight the importance of model architecture, optimizer selection, \n",
    "    and hyperparameter tuning in enhancing the performance of neural networks on classification tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
